{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b31e16",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install httpx feedparser pandas numpy matplotlib seaborn -q\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import asyncio\n",
    "import httpx\n",
    "import feedparser\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f44e3",
   "metadata": {},
   "source": [
    "## 2. Kanyakumari Location Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d232f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kanyakumari coordinates\n",
    "KANYAKUMARI_LAT = 8.0883\n",
    "KANYAKUMARI_LON = 77.5385\n",
    "\n",
    "print(f\"üìç Monitoring Location: Kanyakumari, Tamil Nadu, India\")\n",
    "print(f\"   Latitude: {KANYAKUMARI_LAT}¬∞N\")\n",
    "print(f\"   Longitude: {KANYAKUMARI_LON}¬∞E\")\n",
    "print(f\"   Region: Southernmost tip of Indian Peninsula\")\n",
    "print(f\"   Seas: Arabian Sea, Bay of Bengal, Indian Ocean confluence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af256b7",
   "metadata": {},
   "source": [
    "## 3. Real-Time Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanyakumariOceanMonitor:\n",
    "    \"\"\"Real-time ocean data fetcher for Kanyakumari region.\"\"\"\n",
    "    \n",
    "    def __init__(self, lat: float = KANYAKUMARI_LAT, lon: float = KANYAKUMARI_LON):\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        \n",
    "    def fetch_marine_data(self) -> Dict:\n",
    "        \"\"\"Fetch current marine/wave data from Open-Meteo.\"\"\"\n",
    "        url = \"https://marine-api.open-meteo.com/v1/marine\"\n",
    "        params = {\n",
    "            \"latitude\": self.lat,\n",
    "            \"longitude\": self.lon,\n",
    "            \"current\": \"wave_height,wave_direction,wave_period,swell_wave_height,swell_wave_direction,swell_wave_period\",\n",
    "            \"hourly\": \"wave_height,wave_direction,wave_period\",\n",
    "            \"forecast_days\": 3,\n",
    "            \"timezone\": \"Asia/Kolkata\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = httpx.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Marine data fetch error: {e}\")\n",
    "            return self._mock_marine_data()\n",
    "    \n",
    "    def fetch_weather_data(self) -> Dict:\n",
    "        \"\"\"Fetch weather data from Open-Meteo.\"\"\"\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            \"latitude\": self.lat,\n",
    "            \"longitude\": self.lon,\n",
    "            \"current\": \"temperature_2m,relative_humidity_2m,pressure_msl,wind_speed_10m,wind_direction_10m\",\n",
    "            \"hourly\": \"temperature_2m,pressure_msl,wind_speed_10m\",\n",
    "            \"forecast_days\": 3,\n",
    "            \"timezone\": \"Asia/Kolkata\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = httpx.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Weather data fetch error: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def fetch_earthquakes(self, days: int = 7, min_magnitude: float = 4.0) -> List[Dict]:\n",
    "        \"\"\"Fetch recent earthquakes from USGS.\"\"\"\n",
    "        end_time = datetime.utcnow()\n",
    "        start_time = end_time - timedelta(days=days)\n",
    "        \n",
    "        url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "        params = {\n",
    "            \"format\": \"geojson\",\n",
    "            \"starttime\": start_time.strftime(\"%Y-%m-%d\"),\n",
    "            \"endtime\": end_time.strftime(\"%Y-%m-%d\"),\n",
    "            \"minmagnitude\": min_magnitude,\n",
    "            \"minlatitude\": -10,\n",
    "            \"maxlatitude\": 30,\n",
    "            \"minlongitude\": 60,\n",
    "            \"maxlongitude\": 100,\n",
    "            \"orderby\": \"time\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = httpx.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            earthquakes = []\n",
    "            for feature in data.get(\"features\", [])[:10]:\n",
    "                props = feature[\"properties\"]\n",
    "                coords = feature[\"geometry\"][\"coordinates\"]\n",
    "                earthquakes.append({\n",
    "                    \"magnitude\": props.get(\"mag\"),\n",
    "                    \"place\": props.get(\"place\"),\n",
    "                    \"time\": datetime.fromtimestamp(props.get(\"time\", 0) / 1000).isoformat(),\n",
    "                    \"depth_km\": coords[2] if len(coords) > 2 else None,\n",
    "                    \"latitude\": coords[1],\n",
    "                    \"longitude\": coords[0],\n",
    "                    \"tsunami_flag\": props.get(\"tsunami\", 0)\n",
    "                })\n",
    "            return earthquakes\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Earthquake data fetch error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _mock_marine_data(self) -> Dict:\n",
    "        \"\"\"Generate mock marine data for testing.\"\"\"\n",
    "        return {\n",
    "            \"current\": {\n",
    "                \"wave_height\": np.random.uniform(0.5, 2.5),\n",
    "                \"wave_direction\": np.random.uniform(0, 360),\n",
    "                \"wave_period\": np.random.uniform(4, 12),\n",
    "                \"swell_wave_height\": np.random.uniform(0.3, 1.5),\n",
    "                \"swell_wave_direction\": np.random.uniform(0, 360),\n",
    "                \"swell_wave_period\": np.random.uniform(6, 15)\n",
    "            },\n",
    "            \"hourly\": {\n",
    "                \"time\": [(datetime.now() + timedelta(hours=i)).isoformat() for i in range(72)],\n",
    "                \"wave_height\": [np.random.uniform(0.5, 2.5) for _ in range(72)]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_tsunami_risk(self, marine_data: Dict, earthquakes: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate tsunami risk score based on multiple factors.\"\"\"\n",
    "        risk_score = 0.0\n",
    "        factors = []\n",
    "        \n",
    "        # Wave height factor\n",
    "        current = marine_data.get(\"current\", {})\n",
    "        wave_height = current.get(\"wave_height\", 0)\n",
    "        if wave_height > 3.0:\n",
    "            risk_score += 0.2\n",
    "            factors.append(f\"High waves: {wave_height:.1f}m\")\n",
    "        \n",
    "        # Earthquake factor\n",
    "        for eq in earthquakes[:5]:\n",
    "            mag = eq.get(\"magnitude\", 0)\n",
    "            depth = eq.get(\"depth_km\", 100)\n",
    "            \n",
    "            if mag >= 7.0 and depth < 70:\n",
    "                risk_score += 0.4\n",
    "                factors.append(f\"Major earthquake: M{mag} at {depth}km depth\")\n",
    "            elif mag >= 6.0 and depth < 50:\n",
    "                risk_score += 0.2\n",
    "                factors.append(f\"Significant earthquake: M{mag}\")\n",
    "            \n",
    "            if eq.get(\"tsunami_flag\", 0) == 1:\n",
    "                risk_score += 0.3\n",
    "                factors.append(\"Tsunami flag from USGS\")\n",
    "        \n",
    "        # Determine risk level\n",
    "        if risk_score >= 0.6:\n",
    "            risk_level = \"HIGH\"\n",
    "        elif risk_score >= 0.3:\n",
    "            risk_level = \"MODERATE\"\n",
    "        else:\n",
    "            risk_level = \"LOW\"\n",
    "        \n",
    "        return {\n",
    "            \"risk_score\": min(risk_score, 1.0),\n",
    "            \"risk_level\": risk_level,\n",
    "            \"factors\": factors if factors else [\"Normal conditions\"]\n",
    "        }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = KanyakumariOceanMonitor()\n",
    "print(\"‚úÖ Ocean Monitor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75195f4f",
   "metadata": {},
   "source": [
    "## 4. Fetch Real-Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42198a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all data\n",
    "print(\"üîÑ Fetching real-time data from APIs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "marine_data = monitor.fetch_marine_data()\n",
    "weather_data = monitor.fetch_weather_data()\n",
    "earthquakes = monitor.fetch_earthquakes()\n",
    "tsunami_risk = monitor.calculate_tsunami_risk(marine_data, earthquakes)\n",
    "\n",
    "# Display current conditions\n",
    "current_marine = marine_data.get(\"current\", {})\n",
    "current_weather = weather_data.get(\"current\", {})\n",
    "\n",
    "print(f\"\\nüìä CURRENT CONDITIONS - {datetime.now().strftime('%Y-%m-%d %H:%M:%S IST')}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüåä MARINE DATA:\")\n",
    "print(f\"   Wave Height: {current_marine.get('wave_height', 'N/A')} m\")\n",
    "print(f\"   Wave Period: {current_marine.get('wave_period', 'N/A')} s\")\n",
    "print(f\"   Wave Direction: {current_marine.get('wave_direction', 'N/A')}¬∞\")\n",
    "print(f\"   Swell Height: {current_marine.get('swell_wave_height', 'N/A')} m\")\n",
    "\n",
    "print(f\"\\nüå§Ô∏è WEATHER DATA:\")\n",
    "print(f\"   Temperature: {current_weather.get('temperature_2m', 'N/A')}¬∞C\")\n",
    "print(f\"   Humidity: {current_weather.get('relative_humidity_2m', 'N/A')}%\")\n",
    "print(f\"   Pressure: {current_weather.get('pressure_msl', 'N/A')} hPa\")\n",
    "print(f\"   Wind Speed: {current_weather.get('wind_speed_10m', 'N/A')} km/h\")\n",
    "\n",
    "print(f\"\\nüî¥ TSUNAMI RISK ASSESSMENT:\")\n",
    "print(f\"   Risk Level: {tsunami_risk['risk_level']}\")\n",
    "print(f\"   Risk Score: {tsunami_risk['risk_score']:.2f}\")\n",
    "print(f\"   Factors: {', '.join(tsunami_risk['factors'])}\")\n",
    "\n",
    "print(f\"\\nüåç RECENT EARTHQUAKES ({len(earthquakes)} found):\")\n",
    "for eq in earthquakes[:3]:\n",
    "    print(f\"   M{eq['magnitude']}: {eq['place']} ({eq['time'][:10]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b078afa",
   "metadata": {},
   "source": [
    "## 5. Visualize Wave Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wave height forecast\n",
    "hourly = marine_data.get(\"hourly\", {})\n",
    "times = hourly.get(\"time\", [])[:48]  # Next 48 hours\n",
    "wave_heights = hourly.get(\"wave_height\", [])[:48]\n",
    "\n",
    "if times and wave_heights:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('üåä Kanyakumari Ocean Conditions - 48 Hour Forecast', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Wave height over time\n",
    "    ax1 = axes[0, 0]\n",
    "    hours = range(len(wave_heights))\n",
    "    ax1.fill_between(hours, wave_heights, alpha=0.3, color='blue')\n",
    "    ax1.plot(hours, wave_heights, 'b-', linewidth=2)\n",
    "    ax1.axhline(y=2.0, color='orange', linestyle='--', label='Moderate threshold')\n",
    "    ax1.axhline(y=4.0, color='red', linestyle='--', label='High threshold')\n",
    "    ax1.set_xlabel('Hours from now')\n",
    "    ax1.set_ylabel('Wave Height (m)')\n",
    "    ax1.set_title('Wave Height Forecast')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wave height distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(wave_heights, bins=20, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    ax2.axvline(x=np.mean(wave_heights), color='red', linestyle='--', label=f'Mean: {np.mean(wave_heights):.2f}m')\n",
    "    ax2.set_xlabel('Wave Height (m)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Wave Height Distribution')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Current conditions gauge\n",
    "    ax3 = axes[1, 0]\n",
    "    current_wave = current_marine.get('wave_height', 1.0)\n",
    "    categories = ['Normal\\n(0-1m)', 'Moderate\\n(1-2m)', 'High\\n(2-3m)', 'Extreme\\n(3m+)']\n",
    "    colors = ['green', 'yellow', 'orange', 'red']\n",
    "    values = [1, 1, 1, 1]\n",
    "    \n",
    "    # Determine current category\n",
    "    if current_wave < 1:\n",
    "        highlight = 0\n",
    "    elif current_wave < 2:\n",
    "        highlight = 1\n",
    "    elif current_wave < 3:\n",
    "        highlight = 2\n",
    "    else:\n",
    "        highlight = 3\n",
    "    \n",
    "    bar_colors = ['lightgray'] * 4\n",
    "    bar_colors[highlight] = colors[highlight]\n",
    "    \n",
    "    ax3.bar(categories, values, color=bar_colors, edgecolor='black')\n",
    "    ax3.set_ylim(0, 1.5)\n",
    "    ax3.set_title(f'Current Status: {current_wave:.1f}m')\n",
    "    ax3.set_ylabel('Severity Level')\n",
    "    \n",
    "    # Risk assessment pie\n",
    "    ax4 = axes[1, 1]\n",
    "    risk_score = tsunami_risk['risk_score']\n",
    "    safe_score = 1 - risk_score\n",
    "    \n",
    "    risk_colors = ['green' if risk_score < 0.3 else 'orange' if risk_score < 0.6 else 'red', 'lightgray']\n",
    "    ax4.pie([risk_score, safe_score], labels=['Risk', 'Safe'], colors=risk_colors,\n",
    "            autopct='%1.1f%%', startangle=90, explode=(0.05, 0))\n",
    "    ax4.set_title(f'Tsunami Risk: {tsunami_risk[\"risk_level\"]}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No forecast data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25687b29",
   "metadata": {},
   "source": [
    "## 6. CNN-LSTM Hybrid Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"Custom attention layer for sequence modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, units: int = 64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.u = self.add_weight(\n",
    "            name='attention_context',\n",
    "            shape=(self.units, 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.u, axes=1), axis=1)\n",
    "        context = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "        return context\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "print(\"‚úÖ AttentionLayer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddeaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_backbone(image_shape=(64, 64, 3)):\n",
    "    \"\"\"Build CNN backbone for spatial feature extraction.\"\"\"\n",
    "    inputs = layers.Input(shape=image_shape, name=\"image_input\")\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    return Model(inputs, x, name=\"cnn_backbone\")\n",
    "\n",
    "\n",
    "def build_lstm_backbone(seq_len=24, seq_features=8):\n",
    "    \"\"\"Build LSTM backbone for temporal sequence modeling.\"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, seq_features), name=\"sequence_input\")\n",
    "    \n",
    "    x = layers.Masking(mask_value=0.0)(inputs)\n",
    "    \n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(64, return_sequences=True, dropout=0.2)\n",
    "    )(x)\n",
    "    \n",
    "    # Apply attention\n",
    "    x = AttentionLayer(units=64, name=\"attention\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    return Model(inputs, x, name=\"lstm_backbone\")\n",
    "\n",
    "\n",
    "def build_multimodal_model(\n",
    "    image_shape=(64, 64, 3),\n",
    "    seq_len=24,\n",
    "    seq_features=8,\n",
    "    num_wave_classes=4,\n",
    "    num_tsunami_classes=3\n",
    "):\n",
    "    \"\"\"Build complete multimodal CNN-LSTM hybrid model.\"\"\"\n",
    "    \n",
    "    # Build backbones\n",
    "    cnn = build_cnn_backbone(image_shape)\n",
    "    lstm = build_lstm_backbone(seq_len, seq_features)\n",
    "    \n",
    "    # Get inputs and features\n",
    "    image_input = cnn.input\n",
    "    image_features = cnn.output\n",
    "    \n",
    "    seq_input = lstm.input\n",
    "    seq_features_out = lstm.output\n",
    "    \n",
    "    # Fusion layer\n",
    "    fused = layers.Concatenate(name=\"multimodal_fusion\")([image_features, seq_features_out])\n",
    "    \n",
    "    # Shared dense layers\n",
    "    x = layers.Dense(256, activation='relu')(fused)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output heads\n",
    "    wave_output = layers.Dense(num_wave_classes, activation='softmax', name='wave_severity')(x)\n",
    "    tsunami_output = layers.Dense(num_tsunami_classes, activation='softmax', name='tsunami_risk')(x)\n",
    "    height_output = layers.Dense(1, activation='linear', name='wave_height_meters')(x)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[image_input, seq_input],\n",
    "        outputs=[wave_output, tsunami_output, height_output],\n",
    "        name=\"multimodal_cnn_lstm_hybrid\"\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model builder functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_multimodal_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'wave_severity': 'categorical_crossentropy',\n",
    "        'tsunami_risk': 'categorical_crossentropy',\n",
    "        'wave_height_meters': 'mse'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'wave_severity': 1.0,\n",
    "        'tsunami_risk': 1.5,\n",
    "        'wave_height_meters': 0.5\n",
    "    },\n",
    "    metrics={\n",
    "        'wave_severity': ['accuracy'],\n",
    "        'tsunami_risk': ['accuracy'],\n",
    "        'wave_height_meters': ['mae']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIMODAL CNN-LSTM HYBRID MODEL FOR OCEAN WAVE & TSUNAMI PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5b922",
   "metadata": {},
   "source": [
    "## 7. Generate Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=1000, seq_len=24, seq_features=8):\n",
    "    \"\"\"Generate synthetic training data for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Image data (simulating satellite/heatmap data)\n",
    "    images = np.random.randn(n_samples, 64, 64, 3).astype(np.float32)\n",
    "    \n",
    "    # Sequence data (wave height, pressure, wind, etc.)\n",
    "    sequences = np.random.randn(n_samples, seq_len, seq_features).astype(np.float32)\n",
    "    \n",
    "    # Labels\n",
    "    wave_severity = np.random.randint(0, 4, n_samples)\n",
    "    wave_severity_onehot = tf.keras.utils.to_categorical(wave_severity, 4)\n",
    "    \n",
    "    tsunami_risk = np.random.randint(0, 3, n_samples)\n",
    "    tsunami_risk_onehot = tf.keras.utils.to_categorical(tsunami_risk, 3)\n",
    "    \n",
    "    wave_height = np.random.uniform(0.5, 5.0, n_samples).astype(np.float32)\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'sequences': sequences,\n",
    "        'wave_severity': wave_severity_onehot,\n",
    "        'tsunami_risk': tsunami_risk_onehot,\n",
    "        'wave_height': wave_height\n",
    "    }\n",
    "\n",
    "# Generate data\n",
    "print(\"üîÑ Generating synthetic training data...\")\n",
    "data = generate_synthetic_data(n_samples=2000)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(data['images'])} samples\")\n",
    "print(f\"   Images shape: {data['images'].shape}\")\n",
    "print(f\"   Sequences shape: {data['sequences'].shape}\")\n",
    "print(f\"   Wave severity classes: {data['wave_severity'].shape}\")\n",
    "print(f\"   Tsunami risk classes: {data['tsunami_risk'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d23e7",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "split_idx = int(len(data['images']) * 0.8)\n",
    "\n",
    "train_images = data['images'][:split_idx]\n",
    "train_sequences = data['sequences'][:split_idx]\n",
    "train_wave_severity = data['wave_severity'][:split_idx]\n",
    "train_tsunami_risk = data['tsunami_risk'][:split_idx]\n",
    "train_wave_height = data['wave_height'][:split_idx]\n",
    "\n",
    "val_images = data['images'][split_idx:]\n",
    "val_sequences = data['sequences'][split_idx:]\n",
    "val_wave_severity = data['wave_severity'][split_idx:]\n",
    "val_tsunami_risk = data['tsunami_risk'][split_idx:]\n",
    "val_wave_height = data['wave_height'][split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    [train_images, train_sequences],\n",
    "    {\n",
    "        'wave_severity': train_wave_severity,\n",
    "        'tsunami_risk': train_tsunami_risk,\n",
    "        'wave_height_meters': train_wave_height\n",
    "    },\n",
    "    validation_data=(\n",
    "        [val_images, val_sequences],\n",
    "        {\n",
    "            'wave_severity': val_wave_severity,\n",
    "            'tsunami_risk': val_tsunami_risk,\n",
    "            'wave_height_meters': val_wave_height\n",
    "        }\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27926fc0",
   "metadata": {},
   "source": [
    "## 9. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Model Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Total loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Total Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Wave severity accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history.history['wave_severity_accuracy'], label='Training')\n",
    "ax2.plot(history.history['val_wave_severity_accuracy'], label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Wave Severity Classification Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Tsunami risk accuracy\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history.history['tsunami_risk_accuracy'], label='Training')\n",
    "ax3.plot(history.history['val_tsunami_risk_accuracy'], label='Validation')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Tsunami Risk Classification Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Wave height MAE\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history.history['wave_height_meters_mae'], label='Training')\n",
    "ax4.plot(history.history['val_wave_height_meters_mae'], label='Validation')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('MAE (meters)')\n",
    "ax4.set_title('Wave Height Prediction MAE')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e886d",
   "metadata": {},
   "source": [
    "## 10. Make Predictions with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b18c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_realtime_input(marine_data: Dict, weather_data: Dict, seq_len: int = 24):\n",
    "    \"\"\"Prepare real-time data for model input.\"\"\"\n",
    "    \n",
    "    # Create synthetic image (placeholder for satellite data)\n",
    "    image = np.random.randn(1, 64, 64, 3).astype(np.float32)\n",
    "    \n",
    "    # Create sequence from hourly data\n",
    "    hourly_marine = marine_data.get(\"hourly\", {})\n",
    "    hourly_weather = weather_data.get(\"hourly\", {})\n",
    "    \n",
    "    wave_heights = hourly_marine.get(\"wave_height\", [1.0] * seq_len)[:seq_len]\n",
    "    wave_directions = hourly_marine.get(\"wave_direction\", [180] * seq_len)[:seq_len]\n",
    "    wave_periods = hourly_marine.get(\"wave_period\", [6] * seq_len)[:seq_len]\n",
    "    \n",
    "    temperatures = hourly_weather.get(\"temperature_2m\", [28] * seq_len)[:seq_len]\n",
    "    pressures = hourly_weather.get(\"pressure_msl\", [1013] * seq_len)[:seq_len]\n",
    "    wind_speeds = hourly_weather.get(\"wind_speed_10m\", [15] * seq_len)[:seq_len]\n",
    "    \n",
    "    # Pad if needed\n",
    "    def pad_list(lst, length, default=0):\n",
    "        lst = list(lst) if lst else [default] * length\n",
    "        return lst + [default] * (length - len(lst)) if len(lst) < length else lst[:length]\n",
    "    \n",
    "    wave_heights = pad_list(wave_heights, seq_len, 1.0)\n",
    "    wave_directions = pad_list(wave_directions, seq_len, 180)\n",
    "    wave_periods = pad_list(wave_periods, seq_len, 6)\n",
    "    temperatures = pad_list(temperatures, seq_len, 28)\n",
    "    pressures = pad_list(pressures, seq_len, 1013)\n",
    "    wind_speeds = pad_list(wind_speeds, seq_len, 15)\n",
    "    \n",
    "    # Additional features\n",
    "    humidity = [70] * seq_len\n",
    "    visibility = [10] * seq_len\n",
    "    \n",
    "    # Stack into sequence\n",
    "    sequence = np.array([\n",
    "        wave_heights,\n",
    "        wave_directions,\n",
    "        wave_periods,\n",
    "        temperatures,\n",
    "        pressures,\n",
    "        wind_speeds,\n",
    "        humidity,\n",
    "        visibility\n",
    "    ]).T.astype(np.float32)\n",
    "    \n",
    "    # Normalize\n",
    "    sequence = (sequence - sequence.mean(axis=0)) / (sequence.std(axis=0) + 1e-8)\n",
    "    \n",
    "    return image, sequence.reshape(1, seq_len, 8)\n",
    "\n",
    "# Prepare input from real data\n",
    "image_input, seq_input = prepare_realtime_input(marine_data, weather_data)\n",
    "\n",
    "print(f\"Image input shape: {image_input.shape}\")\n",
    "print(f\"Sequence input shape: {seq_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "predictions = model.predict([image_input, seq_input], verbose=0)\n",
    "\n",
    "wave_probs = predictions[0][0]\n",
    "tsunami_probs = predictions[1][0]\n",
    "wave_height_pred = predictions[2][0][0]\n",
    "\n",
    "WAVE_CLASSES = ['NORMAL', 'MODERATE', 'HIGH', 'EXTREME']\n",
    "TSUNAMI_CLASSES = ['NONE', 'LOW', 'HIGH']\n",
    "\n",
    "wave_class = WAVE_CLASSES[np.argmax(wave_probs)]\n",
    "tsunami_class = TSUNAMI_CLASSES[np.argmax(tsunami_probs)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ AI PREDICTION FOR KANYAKUMARI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüåä WAVE SEVERITY: {wave_class}\")\n",
    "print(f\"   Probabilities:\")\n",
    "for i, cls in enumerate(WAVE_CLASSES):\n",
    "    bar = '‚ñà' * int(wave_probs[i] * 20)\n",
    "    print(f\"     {cls:10s}: {wave_probs[i]*100:5.1f}% {bar}\")\n",
    "\n",
    "print(f\"\\nüî¥ TSUNAMI RISK: {tsunami_class}\")\n",
    "print(f\"   Probabilities:\")\n",
    "for i, cls in enumerate(TSUNAMI_CLASSES):\n",
    "    bar = '‚ñà' * int(tsunami_probs[i] * 20)\n",
    "    print(f\"     {cls:10s}: {tsunami_probs[i]*100:5.1f}% {bar}\")\n",
    "\n",
    "print(f\"\\nüìè PREDICTED WAVE HEIGHT: {wave_height_pred:.2f} meters\")\n",
    "print(f\"   (Current actual: {current_marine.get('wave_height', 'N/A')} meters)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f12b2b",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a62def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('kanyakumari_ocean_model.keras')\n",
    "print(\"‚úÖ Model saved as 'kanyakumari_ocean_model.keras'\")\n",
    "\n",
    "# Download link for Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('kanyakumari_ocean_model.keras')\n",
    "except:\n",
    "    print(\"(Not in Colab environment - model saved locally)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b11e8",
   "metadata": {},
   "source": [
    "## 12. Summary & Next Steps\n",
    "\n",
    "### What We Built:\n",
    "1. **Real-time Data Fetcher** - Fetches live ocean and weather data for Kanyakumari\n",
    "2. **CNN-LSTM Hybrid Model** - Multimodal architecture with attention mechanism\n",
    "3. **Multi-task Predictions** - Wave severity, tsunami risk, and wave height\n",
    "\n",
    "### To Improve:\n",
    "- Use real satellite imagery instead of synthetic data\n",
    "- Train on historical ocean event data\n",
    "- Add more seismic features for better tsunami prediction\n",
    "- Deploy as a web service using FastAPI\n",
    "\n",
    "### APIs Used:\n",
    "- **Open-Meteo Marine API** - Wave data\n",
    "- **Open-Meteo Weather API** - Weather conditions\n",
    "- **USGS Earthquake API** - Seismic monitoring\n",
    "\n",
    "---\n",
    "**üìç Kanyakumari Ocean Wave & Tsunami Prediction System**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
